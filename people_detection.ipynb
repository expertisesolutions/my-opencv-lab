{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "random-smart",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Object Detection\n",
    "\n",
    "This is an initial experiment to learn object detection using OpenCV.\n",
    "\n",
    "The main idea of object detection is to locate the position of an object on some image. Of course, it is not limited to just one object or one type of object and as you will find out, despite we as humans do this task naturally by looking at the image, this is not a simple task algorithmically speaking, by just 'looking' at the images' pixels.\n",
    "\n",
    "The majority of strategies to object detection include the following steps: (1) select a region of interest (ROI), (2) feature extraction, and (3) post-processing. Each of these steps could be executed in many ways, with different techniques, and these differences will distinct each one.\n",
    "\n",
    "The ROI could be a rectangular section of the image, or the entire image itself, it can be every step of a sliding box running through the entire image, or maybe several overlapping boxes with different sizes. Broadly speaking it is just a method to select a section of the image to be compared with some defined pattern that describes the object you are looking for.\n",
    "\n",
    "The object you are locating needs to be described by a pattern defined by a set of features. A feature is a way to abstract some possibly complex mathematical relations between the analyzed pixels, like the two sharp edges of the nose when looking at a front human face. The feature extraction is a vast subject itself and has a lot of different proposals for each type of objects, but basically, you need a method to extract those features directly from a region of pixels, like in face recognition, by using two white rectangles to identify the nose and eyes pattern ([see Haar-like features](https://en.wikipedia.org/wiki/Haar-like_feature)). But there are methods more efficient and easier to use. For a good overview about this, check out the article [Feature Extraction for Object Recognition and Image Classification, by Aastha Tiwari, Anil Kumar Goswami, and Mansi Saraswat](https://www.ijert.org/research/feature-extraction-for-object-recognition-and-image-classification-IJERTV2IS100491.pdf).\n",
    "\n",
    "Obviously, with the advance of the current computational power with common devices, it is very suitable to program software to do an optimization of the parameters of the feature extraction model based on a set of data, like finding the best dimensions for those Haar-like features using machine learning. We also can use a deep neural network (DNN) to model and optimize the feature extraction entirely, in a way that we don't need to engineer the feature by 'hand', being able to work with high a more high level of abstraction.\n",
    "\n",
    "By speaking of DNN, as the most common operation with image processing is the convolution, the class of DNN used for dealing with image processing is mostly named convolutional neuron network (CNN), then we can imagine a ton of matrix operations being processed here. [Read more about this achitecture](https://en.wikipedia.org/wiki/Convolutional_neural_network#Architecture).\n",
    "\n",
    "The following sections will be experimenting by applying some object detection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-raising",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Detecting People w/ Haar Cascade Classifier\n",
    "\n",
    "The idea here is to use a Haar Cascade Classifier algorithm by loading an XML file with the pre-trained parameters for full-body detection.\n",
    "\n",
    "To understand what is Haar Cascade Classifier and how it works, there is a good basic overview from [OpenCV-Python Tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html). A more in-depth looking can be read from [Wikipedia's article Viola-Jones object detection framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework).\n",
    "\n",
    "The approach here is very simple:\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the current frame's image.\n",
    "- Run a full-body pre-trained Haar Cascade Classifier on that image, that should return a list of detected full bodies in a form of a rectangle.\n",
    "- Draw the rectangles on the image.\n",
    "- Display the image.\n",
    "- Save the processed frames into the output video.\n",
    "\n",
    "There are several similar tutorials with almost identical example codes. I particularly followed the tutorial [Computer Vision â€” Detecting objects using Haar Cascade Classifier, from Towards Data Science](https://towardsdatascience.com/computer-vision-detecting-objects-using-haar-cascade-classifier-4585472829a9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demographic-spank",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame width: 640\n",
      "Frame width: 360\n",
      "Video fps: 25.0\n",
      "Processing \"./360p_TownCentreXVID.mp4\" (7502 frames)...\n",
      "Done!\n",
      "1000 frames processed in 20.32015109062195 seconds.\n",
      "(0.02032015109062195) seconds per frame.\n",
      "(49.2122325045858) frames per second.\n",
      "Output saved to \"./output.mp4\".\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2  # or opencv-python\n",
    "import time\n",
    "\n",
    "# Create our body classifier\n",
    "detector = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + 'haarcascade_fullbody.xml'\n",
    "#     cv2.data.haarcascades + 'haarcascade_upperbody.xml'\n",
    ")\n",
    "\n",
    "# Open the input video capture\n",
    "#input_filename = './1080p_TownCentreXVID.mp4'\n",
    "#input_filename = './720p_TownCentreXVID.mp4'\n",
    "#input_filename = './480p_TownCentreXVID.mp4'\n",
    "input_filename = './360p_TownCentreXVID.mp4'\n",
    "# input_filename = '../videos/video_F_2.mp4'\n",
    "vcap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "n_frames = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(\"Frame width:\", frame_width)\n",
    "print(\"Frame width:\", frame_height)\n",
    "print(\"Video fps:\", fps)\n",
    "\n",
    "# Setup the output video file\n",
    "output_filename = './output.mp4'\n",
    "apiPreference = cv2.CAP_FFMPEG\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vout = cv2.VideoWriter(\n",
    "    filename=output_filename,\n",
    "    apiPreference=apiPreference,\n",
    "    fourcc=fourcc,\n",
    "    fps=fps,\n",
    "    frameSize=(frame_width, frame_height),\n",
    "    params=[]\n",
    ")\n",
    "\n",
    "print(f\"Processing \\\"{input_filename}\\\" ({int(n_frames)} frames)...\")\n",
    "\n",
    "# Start app\n",
    "window_name = \"People Detecting\"\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(window_name)\n",
    "\n",
    "# Loop each frame\n",
    "frame_count = 0\n",
    "frames_to_process = 1000\n",
    "processed_frames = np.zeros(frames_to_process, dtype=object)\n",
    "\n",
    "green = (0, 255, 0)\n",
    "red = (255, 0 ,0)\n",
    "\n",
    "# start timer\n",
    "start = time.time()\n",
    "fps_timer = [0, cv2.getTickCount()]\n",
    "while vcap.isOpened():\n",
    "    # Read a frame\n",
    "    ret, frame = vcap.read()\n",
    "    if not ret or frame_count == frames_to_process:\n",
    "        break\n",
    "\n",
    "    # Apply the body classifier\n",
    "    bodies = detector.detectMultiScale(frame, 1.1, 3)\n",
    "\n",
    "    # Extract bounding boxes for any bodies identified\n",
    "    for (x, y, w, h) in bodies:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Compute and put FPS on frame\n",
    "    fps = cv2.getTickFrequency() / (fps_timer[1] - fps_timer[0]);\n",
    "    fps_timer[0] = fps_timer[1]\n",
    "    fps_timer[1] = cv2.getTickCount()\n",
    "    cv2.putText(frame,\n",
    "        text=f\"FPS: {int(fps)}\",\n",
    "        org=(frame_width -60, frame_height -5),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.3,\n",
    "        color=green,\n",
    "        thickness=1\n",
    "    );\n",
    "        \n",
    "    # Save frame\n",
    "    processed_frames[frame_count] = frame\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show in app\n",
    "    cv2.imshow(window_name, frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# end timer\n",
    "end = time.time()\n",
    "overall_elapsed_time = end - start\n",
    "elapsed_time_per_frame = overall_elapsed_time / frame_count\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"{frame_count} frames processed in {overall_elapsed_time} seconds.\")\n",
    "print(f\"({elapsed_time_per_frame}) seconds per frame.\")\n",
    "print(f\"({1/elapsed_time_per_frame}) frames per second.\")\n",
    "\n",
    "# Write processed frames to file\n",
    "for frame in processed_frames:\n",
    "    vout.write(frame)\n",
    "\n",
    "print(f\"Output saved to \\\"{output_filename}\\\".\")\n",
    "\n",
    "vcap.release()\n",
    "vout.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-cable",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Detecting people with Background Subtractor\n",
    "\n",
    "The idea here is that if the camera is static, we can take advantage of that and use a background subtractor algorithm to get a mask of the moving regions, then filter it and locate the blobs. This should give us the moving objects on a static camera.\n",
    "\n",
    "The OpenCV page has a good article called [How to Use Background Subtraction Methods](https://docs.opencv.org/master/d1/dc5/tutorial_background_subtraction.html) commenting on the basics. To understand how they work and to get a performance comparison, be sure to read the article [A Comparison between Background Modelling Methods for Vehicle Segmentation in Highway Traffic Videos, by L. A. Marcomini and A. L. Cunha](https://arxiv.org/pdf/1810.02835.pdf).\n",
    "\n",
    "The approach here is also very simple:  \n",
    "- Initialize the detector as a background subtractor.\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the current frame's image.\n",
    "- Apply the detector to the image, getting a mask with the background extracted (the background is black, the non-background is white).\n",
    "- Filter with Erode, Dilate and Close to get a better separation of the detections.\n",
    "- Find each blob in the image and get its bounding box.\n",
    "- Draw the bounding box as rectangles on the image.\n",
    "- Display the image.\n",
    "- Save the processed frames into the output video.\n",
    "\n",
    "There are some tutorials around this and I particularly followed the [Object Tracking with Opencv and Python, from PySource](https://pysource.com/2021/01/28/object-tracking-with-opencv-and-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "right-elder",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame width: 640\n",
      "Frame width: 360\n",
      "Video fps: 25.0\n",
      "Processing \"./360p_TownCentreXVID.mp4\" (7502 frames)...\n",
      "Done!\n",
      "1000 frames processed in 6.298488616943359 seconds.\n",
      "(0.00629848861694336) seconds per frame.\n",
      "(158.76824756179323) frames per second.\n",
      "Output saved to \"./output.mp4\".\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2  # or opencv-python\n",
    "import time\n",
    "\n",
    "# Create our body classifier\n",
    "detector = cv2.createBackgroundSubtractorMOG2(history=150, varThreshold=50)\n",
    "\n",
    "# Open the input video capture\n",
    "#input_filename = './1080p_TownCentreXVID.mp4'\n",
    "#input_filename = './720p_TownCentreXVID.mp4'\n",
    "#input_filename = './480p_TownCentreXVID.mp4'\n",
    "input_filename = './360p_TownCentreXVID.mp4'\n",
    "# input_filename = '../videos/video_F_2.mp4'\n",
    "vcap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "n_frames = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(\"Frame width:\", frame_width)\n",
    "print(\"Frame width:\", frame_height)\n",
    "print(\"Video fps:\", fps)\n",
    "\n",
    "# Setup the output video file\n",
    "output_filename = './output.mp4'\n",
    "apiPreference = cv2.CAP_FFMPEG\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vout = cv2.VideoWriter(\n",
    "    filename=output_filename,\n",
    "    apiPreference=apiPreference,\n",
    "    fourcc=fourcc,\n",
    "    fps=fps,\n",
    "    frameSize=(frame_width, frame_height),\n",
    "    params=[]\n",
    ")\n",
    "\n",
    "print(f\"Processing \\\"{input_filename}\\\" ({int(n_frames)} frames)...\")\n",
    "\n",
    "# Start app\n",
    "window_name = \"People Detecting\"\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(window_name)\n",
    "\n",
    "# Loop each frame\n",
    "frame_count = 0\n",
    "frames_to_process = 1000\n",
    "processed_frames = np.zeros(frames_to_process, dtype=object)\n",
    "\n",
    "green = (0, 255, 0)\n",
    "red = (255, 0 ,0)\n",
    "\n",
    "# start timer\n",
    "start = time.time()\n",
    "fps_timer = [0, cv2.getTickCount()]\n",
    "while vcap.isOpened():\n",
    "    # Read a frame\n",
    "    ret, frame = vcap.read()\n",
    "    if not ret or frame_count == frames_to_process:\n",
    "        break\n",
    "\n",
    "    # Filter image to get people blobs\n",
    "    mask = detector.apply(frame)\n",
    "    mask[mask>125] = 255\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 7))\n",
    "    mask = cv2.erode(mask, kernel, iterations=1)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=5)   \n",
    "\n",
    "    # Consider each blob a person\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bodies = []\n",
    "    for countour in contours:\n",
    "        # Calculate area and remove small elements\n",
    "        area = cv2.contourArea(countour)\n",
    "        if area > 100:\n",
    "            x, y, w, h = cv2.boundingRect(countour)\n",
    "            bodies += [(x, y, w, h)]\n",
    "\n",
    "    # Reconstruct the colors\n",
    "    frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "    \n",
    "    # Extract bounding boxes for any bodies identified\n",
    "    for (x, y, w, h) in bodies:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "    # Compute and put FPS on frame\n",
    "    fps = cv2.getTickFrequency() / (fps_timer[1] - fps_timer[0]);\n",
    "    fps_timer[0] = fps_timer[1]\n",
    "    fps_timer[1] = cv2.getTickCount()\n",
    "    cv2.putText(frame,\n",
    "        text=f\"FPS: {int(fps)}\",\n",
    "        org=(frame_width -60, frame_height -5),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.3,\n",
    "        color=green,\n",
    "        thickness=1\n",
    "    );\n",
    "\n",
    "    # Save frame\n",
    "    processed_frames[frame_count] = frame\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show in app\n",
    "    cv2.imshow(window_name, frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# end timer\n",
    "end = time.time()\n",
    "overall_elapsed_time = end - start\n",
    "elapsed_time_per_frame = overall_elapsed_time / frame_count\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"{frame_count} frames processed in {overall_elapsed_time} seconds.\")\n",
    "print(f\"({elapsed_time_per_frame}) seconds per frame.\")\n",
    "print(f\"({1/elapsed_time_per_frame}) frames per second.\")\n",
    "\n",
    "# Write processed frames to file\n",
    "for frame in processed_frames:\n",
    "    vout.write(frame)\n",
    "\n",
    "print(f\"Output saved to \\\"{output_filename}\\\".\")\n",
    "\n",
    "vcap.release()\n",
    "vout.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-continent",
   "metadata": {},
   "source": [
    "# Detecting people using YOLO\n",
    "\n",
    "YOLO is the idea that... You Only Look Once is the idea that a CNN can process the entire image at once instead of running a detection on a sliding window through the image or a bunch of selective ROI. Instead of that, it divides the image into an SxS grid that is processed in one-pass the entire image.\n",
    "\n",
    "For each cell, a defined number of objects can be detected by using something referred to as _Dimension Clusters_, that is similar to _anchor boxes_, but has its dimensions pre-fitted to best match the best [IOU](https://en.wikipedia.org/wiki/Jaccard_index) with the bounding boxes found in the training dataset. Each detected object will have relative dimensions offset from the cluster/box used for that detection and coordinate offsets relative to the corresponding cell of the grid.\n",
    "\n",
    "Each cell contains attributes regarding each bounding box's (1) position, (2) dimension, and (3) confidence based on IOU. Also, each cell contains the list of the probability of each object classes that can be identified (3 values if trained to detect person, cat, and dog).\n",
    "\n",
    "YOLO (since version 2) trains with a random multi-scaling pre-processing. Note here that because of the nature of the non-fully connected ConvNets, the weights don't need to be changed when resizing the features. This multi-scaling process not only improves its overall precision but it means it can process images with different sizes while also gives it a parameter as a trade-off between speed (FPS) and accuracy that can be adjusted in runtime, that using OpenCV it would be referred to as `size` in the `blobFromImage()`, responsible for normalizing the input image to the DNN (read more (here)[https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works]).\n",
    "\n",
    "The image is then processed in a single convolutional network and outputs a list of bounding boxes relative to the image size.\n",
    "\n",
    "As an open-source code, currently, there are several YOLO releases, some newer from different authors, so it is quite confusing to understand, but fortunately, [Towards Data Science](https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109) published a good overview from version 1 to 5. Here we'll be experimenting with Darknet's YOLOv3 and AlexeyAB's YOLOv4.\n",
    "\n",
    "The approach here is also very simple:  \n",
    "- Initialize the detector as a background subtractor.\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the current frame's image.\n",
    "- Apply the detector to the image, getting a mask with the background extracted (the background is black, the non-background is white).\n",
    "- Filter with Erode, Dilate and Close to get a better separation of the detections.\n",
    "- Find each blob in the image and get its bounding box.\n",
    "- Draw the bounding box as rectangles on the image.\n",
    "- Display the image.\n",
    "- Save the processed frames into the output video.\n",
    "\n",
    "Refs:\n",
    " - [YOLOv1](https://pjreddie.com/media/files/papers/yolo_1.pdf)\n",
    " - [YOLOv2/YOLO9000](https://pjreddie.com/media/files/papers/YOLO9000.pdf)\n",
    " - [YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n",
    " - https://learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c\n",
    " - https://pysource.com/2019/06/27/yolo-object-detection-using-opencv-with-python\n",
    " - https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works\n",
    " - https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html\n",
    " - https://blog.roboflow.com/yolov5-improvements-and-evaluation/\n",
    "\n",
    "Downloads:\n",
    " - [Darknet models](https://pjreddie.com/darknet/imagenet/#pretrained)\n",
    "  - [names-file](https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names)\n",
    "  - YOLOv3:\n",
    "    - [cfg-file](https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg)\n",
    "    - [weights-file](https://pjreddie.com/media/files/yolov3.weights)\n",
    "  - YOLOv3-tiny:\n",
    "    - [cfg-file](https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3-tiny.cfg)\n",
    "    - [weights-file](https://pjreddie.com/media/files/yolov3-tiny.weights)\n",
    " - https://github.com/kiyoshiiriemon/yolov4_darknet\n",
    "  - [names-file](https://raw.githubusercontent.com/AlexeyAB/darknet/darknet_yolo_v4_pre/cfg/coco.names)\n",
    "  - YOLOv4:\n",
    "    - [cfg-file](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.cfg)\n",
    "     - [weights-file](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "  - YOLOv4-tiny:\n",
    "    - [cfg-file](https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg)\n",
    "    - [weights-file](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "practical-reconstruction",
   "metadata": {
    "code_folding": [
     63,
     111
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 classes loaded: ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "color: [[247.86325633 136.56791797 101.88263367]\n",
      " [187.9258441  226.0536722    3.25159206]\n",
      " [174.71940172  72.65114082  91.69779527]\n",
      " [ 15.59758265  23.28065927  85.48506208]\n",
      " [239.78619528 133.3720869    8.58793549]\n",
      " [106.00258423  33.42050253  61.31424871]\n",
      " [ 27.31008061  79.96261338  16.10929611]\n",
      " [ 74.6417272  112.37177599  97.18420651]\n",
      " [ 34.84313483 225.09497804  34.38477141]\n",
      " [ 18.17366013  59.65083809 224.43811825]\n",
      " [ 72.53171808  40.55053051 229.20193761]\n",
      " [174.23116778 253.8053355  241.78297125]\n",
      " [  9.25391761 169.39910559 134.98364626]\n",
      " [232.96371984 197.58381337  25.2101099 ]\n",
      " [216.66214273 211.13208954 186.13126829]\n",
      " [149.65690191 183.94363942 136.5390712 ]\n",
      " [227.44886081  59.36134546 177.79009595]\n",
      " [110.75047343 187.97052214 214.37981643]\n",
      " [146.61915474  75.73736706 107.27593243]\n",
      " [178.39422132  21.22938372 158.56842619]\n",
      " [158.31246963 225.13322654 134.00132609]\n",
      " [230.69453729 220.38303554 138.18832645]\n",
      " [ 89.15229642  17.63484159  20.28892073]\n",
      " [164.97472015  17.88925639 216.76853296]\n",
      " [ 75.48189862  23.33040084 109.50258365]\n",
      " [ 46.66142779 245.26824862  41.06676161]\n",
      " [  9.18407942 128.76032908 217.7108254 ]\n",
      " [247.6696817   29.0603994  237.23037786]\n",
      " [221.49068202  31.29254514  27.95165819]\n",
      " [207.95162188   1.70151223 171.22731557]\n",
      " [121.43491893 206.43324039  68.05244447]\n",
      " [241.99108472  78.07786445  63.42810336]\n",
      " [137.15500705  74.32974726 202.40871365]\n",
      " [ 86.87259732 140.32700763 201.77329764]\n",
      " [ 99.06963103  43.687732   186.07052654]\n",
      " [ 32.54685062 168.2148026  248.30959299]\n",
      " [ 49.84064916 241.6196688  253.77892136]\n",
      " [126.93340714  29.04560885 188.72769552]\n",
      " [103.08155415 229.15065354 116.62509995]\n",
      " [ 73.24112918 178.33973797 142.55234812]\n",
      " [136.93941438 241.93014358 123.06936711]\n",
      " [151.04443586 248.05855634 176.15481346]\n",
      " [195.38376987  52.50888651  47.20694568]\n",
      " [179.35497748 173.73417118  78.75151213]\n",
      " [239.41252605   5.07623867  85.75141056]\n",
      " [ 43.97807804 152.7264385   29.41286345]\n",
      " [ 27.09417759 106.69353873 142.27114437]\n",
      " [234.05208826 160.14763434 110.53646584]\n",
      " [ 74.036615   208.29061425  29.94264801]\n",
      " [  1.32201703  50.05513024 133.93774757]\n",
      " [162.36358792  49.61407805 196.17521   ]\n",
      " [104.03870271  68.97830469  53.29831095]\n",
      " [ 68.53757489  19.55532497 234.47948848]\n",
      " [122.72300533 195.42182735 192.98401574]\n",
      " [167.53657195 165.1486322  198.62645318]\n",
      " [ 26.38168382 153.0220138  114.90275563]\n",
      " [254.1006876  251.25573593 120.94273634]\n",
      " [203.46120732  18.39085213 147.90713576]\n",
      " [223.66788091  59.87146075  92.37096993]\n",
      " [107.48160264 186.24751062  98.94838942]\n",
      " [ 67.48676509 114.6020301  220.45024601]\n",
      " [230.47420227 172.78345435 224.23489531]\n",
      " [193.71909309 183.07919162 194.59353524]\n",
      " [ 17.71422476 242.56016625 139.87163932]\n",
      " [195.53295779 155.55301688 142.81405374]\n",
      " [ 30.4069229  122.25846679 223.96745346]\n",
      " [158.98362097 237.52303983  65.79771573]\n",
      " [ 12.34135727 139.48994761  76.2076621 ]\n",
      " [219.30258364 110.67618087 218.08423369]\n",
      " [248.28351225 209.50990225 236.14606051]\n",
      " [ 37.95012012 161.65141814 233.58844842]\n",
      " [102.73784736  50.01807024 234.14268077]\n",
      " [ 83.91672774 161.54866171 176.53042562]\n",
      " [248.91339006   6.57359406 115.30092559]\n",
      " [211.38546145 168.78863311  89.99282729]\n",
      " [100.28307968  68.60598738 252.91910784]\n",
      " [ 56.22345696 150.56892473 191.03054825]\n",
      " [177.90410416  51.29085789  59.55881196]\n",
      " [199.40628585 136.7115009  205.8613906 ]\n",
      " [  7.70101307 215.42582316  34.49079705]]\n",
      "Frame width: 640\n",
      "Frame width: 360\n",
      "Video channels: 0\n",
      "Video fps: 25.0\n",
      "Processing \"./360p_TownCentreXVID.mp4\" (7502 frames)...\n",
      "Done!\n",
      "1000 frames processed in 25.561288833618164 seconds.\n",
      "(0.025561288833618166) seconds per frame.\n",
      "(39.12165800829266) frames per second.\n",
      "Output saved to \"./output.mp4\".\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2  # or opencv-python\n",
    "import time\n",
    "\n",
    "# Define YOLO files to load\n",
    "# weights_file, cfg_file, names_file = \"pretrained_yolo/v3/yolov3-tiny.weights\", \"pretrained_yolo/v3/yolov3-tiny.cfg\", \"pretrained_yolo/v3/coco.names\"\n",
    "weights_file, cfg_file, names_file = \"pretrained_yolo/v4/yolov4-tiny.weights\", \"pretrained_yolo/v4/yolov4-tiny.cfg\", \"pretrained_yolo/v4/coco.names\"\n",
    "\n",
    "# YOLO and DNN Configs\n",
    "# ref: https://docs.opencv.org/4.5.1/d6/d0f/group__dnn.html\n",
    "mean = (0, 0, 0)  # YOLO doesn't use subtraction.\n",
    "scale_factor = 1 / (255)  # the colorspace is normalized to match values from 0 to 1, so for 8 bits depth it should be 1/255.\n",
    "blob_size = tuple([128*2]*2)  # this will impacat on precision over FPS. Smaller means faster. 320x320 is a common value.\n",
    "confidence_threshold = 0.3  # 0 means no threshold. 0.5 is a common value.\n",
    "supression_threshold = 0.4  # 1 means no supression. 0.4 is a common value.\n",
    "\n",
    "dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_CPU, cv2.dnn.DNN_BACKEND_DEFAULT\n",
    "# dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_CPU, cv2.dnn.DNN_BACKEND_OPENCV\n",
    "# dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_OPENCL, cv2.dnn.DNN_BACKEND_DEFAULT\n",
    "# dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_OPENCL, cv2.dnn.DNN_BACKEND_OPENCV\n",
    "# dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_CUDA, cv2.dnn.DNN_BACKEND_CUDA\n",
    "# dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_VULKAN, cv2.dnn.DNN_BACKEND_VKCOM\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(weights_file, cfg_file)\n",
    "net.setPreferableBackend(dnn_backend)\n",
    "net.setPreferableTarget(dnn_target)\n",
    "\n",
    "classes = []\n",
    "with open(names_file, \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    print(f\"{len(classes)} classes loaded:\", end=' ')\n",
    "    print(classes)\n",
    "layer_names = net.getLayerNames()\n",
    "# output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "output_layers = net.getUnconnectedOutLayersNames()\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# Open the input video capture\n",
    "#input_filename = './1080p_TownCentreXVID.mp4'\n",
    "#input_filename = './720p_TownCentreXVID.mp4'\n",
    "#input_filename = './480p_TownCentreXVID.mp4'\n",
    "input_filename = './360p_TownCentreXVID.mp4'\n",
    "# input_filename = '../videos/video_F_2.mp4'\n",
    "vcap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "channels = int(vcap.get(cv2.CAP_PROP_CHANNEL))\n",
    "fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "n_frames = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(\"Frame width:\", frame_width)\n",
    "print(\"Frame width:\", frame_height)\n",
    "print(\"Video channels:\", channels)\n",
    "print(\"Video fps:\", fps)\n",
    "\n",
    "# Setup the output video file\n",
    "output_filename = './output.mp4'\n",
    "apiPreference = cv2.CAP_FFMPEG\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vout = cv2.VideoWriter(\n",
    "    filename=output_filename,\n",
    "    apiPreference=apiPreference,\n",
    "    fourcc=fourcc,\n",
    "    fps=fps,\n",
    "    frameSize=(frame_width, frame_height),\n",
    "    params=[]\n",
    ")\n",
    "print(f\"Processing \\\"{input_filename}\\\" ({int(n_frames)} frames)...\")\n",
    "\n",
    "# Start app\n",
    "window_name = \"People Detecting\"\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(window_name)\n",
    "\n",
    "# Loop each frame\n",
    "frame_count = 0\n",
    "frames_to_process = 1000\n",
    "processed_frames = np.zeros(frames_to_process, dtype=object)\n",
    "\n",
    "green = (0, 255, 0)\n",
    "red = (255, 0 ,0)\n",
    "\n",
    "# start timer\n",
    "start = time.time()\n",
    "fps_timer = [0, cv2.getTickCount()]\n",
    "while vcap.isOpened():\n",
    "    # Read a frame\n",
    "    ret, frame = vcap.read()\n",
    "    if not ret or frame_count == frames_to_process:\n",
    "        break\n",
    "\n",
    "    # Detecting objects\n",
    "    blob = cv2.dnn.blobFromImage(frame,\n",
    "        scalefactor=scale_factor,\n",
    "        size=blob_size,\n",
    "        mean=mean,\n",
    "        swapRB=True,\n",
    "        crop=False,\n",
    "#         ddepth=cv2.CV_32F\n",
    "    )\n",
    "    net.setInput(blob)\n",
    "    output_blobs = net.forward(output_layers)\n",
    "\n",
    "    # Extract bounding boxes for any object detected\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for output_blob in output_blobs:\n",
    "        for detection in output_blob:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > confidence_threshold:\n",
    "                center_x, center_y, w, h = detection[:4] * np.array(\n",
    "                    [frame_width, frame_height, frame_width, frame_height])\n",
    "                x = center_x - (w / 2)\n",
    "                y = center_y - (h / 2)\n",
    "                boxes.append([int(x), int(y), int(w), int(h)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Remove coincidental detections\n",
    "    indices = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes, \n",
    "        scores=confidences, \n",
    "        score_threshold=confidence_threshold, \n",
    "        nms_threshold=supression_threshold\n",
    "    )\n",
    "\n",
    "    # Showing informations on the screen\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = classes[class_ids[i]]\n",
    "            confidence = round(confidences[i]*100)\n",
    "            color = colors[class_ids[i]]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                text=f\"{label} : {confidence}%\",\n",
    "                org=(x, y-5),\n",
    "                fontFace=font,\n",
    "                fontScale=0.3,\n",
    "                color=color,\n",
    "                thickness=1\n",
    "            )\n",
    "        \n",
    "    # Compute and put FPS on frame\n",
    "    fps = cv2.getTickFrequency() / (fps_timer[1] - fps_timer[0]);\n",
    "    fps_timer[0] = fps_timer[1]\n",
    "    fps_timer[1] = cv2.getTickCount()\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        text=f\"FPS: {int(fps)}\",\n",
    "        org=(frame_width -60, frame_height -5),\n",
    "        fontFace=font,\n",
    "        fontScale=0.3,\n",
    "        color=green,\n",
    "        thickness=1\n",
    "    );\n",
    "\n",
    "    # Save frame\n",
    "    processed_frames[frame_count] = frame\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show in app\n",
    "    cv2.imshow(window_name, frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# end timer\n",
    "end = time.time()\n",
    "overall_elapsed_time = end - start\n",
    "elapsed_time_per_frame = overall_elapsed_time / frame_count\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"{frame_count} frames processed in {overall_elapsed_time} seconds.\")\n",
    "print(f\"({elapsed_time_per_frame}) seconds per frame.\")\n",
    "print(f\"({1/elapsed_time_per_frame}) frames per second.\")\n",
    "\n",
    "# Write processed frames to file\n",
    "for frame in processed_frames:\n",
    "    vout.write(frame)\n",
    "\n",
    "print(f\"Output saved to \\\"{output_filename}\\\".\")\n",
    "\n",
    "vcap.release()\n",
    "vout.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-combination",
   "metadata": {},
   "source": [
    "next: \n",
    "- Scaled-YOLOv4?\n",
    "- Training with custom dataset?\n",
    "- YOLOv5 with PyTorch?\n",
    "- Semantic Segmentation?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
