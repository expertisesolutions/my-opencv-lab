{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "random-smart",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Object Detection\n",
    "\n",
    "This is an initial experiment to learn object detection using OpenCV.\n",
    "\n",
    "The main idea of object detection is to locate the position of an object on some image. Of course, it is not limited to just one object or one type of object and as you will find out, despite we as humans do this task naturally by looking at the image, this is not a simple task algorithmically speaking, by just 'looking' at the images' pixels.\n",
    "\n",
    "The majority of strategies to object detection include the following steps: (1) select a region of interest (ROI), (2) feature extraction, and (3) post-processing. Each of these steps could be executed in many ways, with different techniques, and these differences will distinct each one.\n",
    "\n",
    "The ROI could be a rectangular section of the image, or the entire image itself, it can be every step of a sliding box running through the entire image, or maybe several overlapping boxes with different sizes. Broadly speaking it is just a method to select a section of the image to be compared with some defined pattern that describes the object you are looking for.\n",
    "\n",
    "The object you are locating needs to be described by a pattern defined by a set of features. A feature is a way to abstract some possibly complex mathematical relations between the analyzed pixels, like the two sharp edges of the nose when looking at a front human face. The feature extraction is a vast subject itself and has a lot of different proposals for each type of objects, but basically, you need a method to extract those features directly from a region of pixels, like in face recognition, by using two white rectangles to identify the nose and eyes pattern ([see Haar-like features](https://en.wikipedia.org/wiki/Haar-like_feature)). But there are methods more efficient and easier to use. For a good overview about this, check out the article [Feature Extraction for Object Recognition and Image Classification, by Aastha Tiwari, Anil Kumar Goswami, and Mansi Saraswat](https://www.ijert.org/research/feature-extraction-for-object-recognition-and-image-classification-IJERTV2IS100491.pdf).\n",
    "\n",
    "Obviously, with the advance of the current computational power with common devices, it is very suitable to program software to do an optimization of the parameters of the feature extraction model based on a set of data, like finding the best dimensions for those Haar-like features using machine learning. We also can use a deep neural network (DNN) to model and optimize the feature extraction entirely, in a way that we don't need to engineer the feature by 'hand', being able to work with high a more high level of abstraction.\n",
    "\n",
    "By speaking of DNN, as the most common operation with image processing is the convolution, the class of DNN used for dealing with image processing is mostly named convolutional neuron network (CNN), then we can imagine a ton of matrix operations being processed here. [Read more about this achitecture](https://en.wikipedia.org/wiki/Convolutional_neural_network#Architecture).\n",
    "\n",
    "The following sections will be experimenting by applying some object detection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-migration",
   "metadata": {},
   "source": [
    "## Getting our ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sorted-jonathan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Left</th>\n",
       "      <th>Top</th>\n",
       "      <th>Right</th>\n",
       "      <th>Bottom</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>235.925</td>\n",
       "      <td>770.142</td>\n",
       "      <td>371.546</td>\n",
       "      <td>1101.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <td>285.748</td>\n",
       "      <td>291.418</td>\n",
       "      <td>370.556</td>\n",
       "      <td>493.414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <td>286.849</td>\n",
       "      <td>230.501</td>\n",
       "      <td>365.794</td>\n",
       "      <td>416.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>3</th>\n",
       "      <td>719.708</td>\n",
       "      <td>220.830</td>\n",
       "      <td>786.893</td>\n",
       "      <td>408.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <th>4</th>\n",
       "      <td>793.722</td>\n",
       "      <td>235.410</td>\n",
       "      <td>861.430</td>\n",
       "      <td>427.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47742</th>\n",
       "      <th>3089</th>\n",
       "      <th>154</th>\n",
       "      <td>810.452</td>\n",
       "      <td>55.174</td>\n",
       "      <td>862.087</td>\n",
       "      <td>196.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47743</th>\n",
       "      <th>3089</th>\n",
       "      <th>155</th>\n",
       "      <td>1097.855</td>\n",
       "      <td>30.190</td>\n",
       "      <td>1144.805</td>\n",
       "      <td>165.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47744</th>\n",
       "      <th>3089</th>\n",
       "      <th>156</th>\n",
       "      <td>1061.762</td>\n",
       "      <td>25.935</td>\n",
       "      <td>1108.763</td>\n",
       "      <td>160.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47745</th>\n",
       "      <th>3089</th>\n",
       "      <th>157</th>\n",
       "      <td>1804.041</td>\n",
       "      <td>175.430</td>\n",
       "      <td>1892.821</td>\n",
       "      <td>361.565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47746</th>\n",
       "      <th>3090</th>\n",
       "      <th>139</th>\n",
       "      <td>1491.499</td>\n",
       "      <td>172.528</td>\n",
       "      <td>1555.734</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47747 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Left      Top     Right    Bottom\n",
       "Index Frame Id                                        \n",
       "0     0     0     235.925  770.142   371.546  1101.029\n",
       "1     0     1     285.748  291.418   370.556   493.414\n",
       "2     0     2     286.849  230.501   365.794   416.410\n",
       "3     0     3     719.708  220.830   786.893   408.145\n",
       "4     0     4     793.722  235.410   861.430   427.627\n",
       "...                   ...      ...       ...       ...\n",
       "47742 3089  154   810.452   55.174   862.087   196.133\n",
       "47743 3089  155  1097.855   30.190  1144.805   165.622\n",
       "47744 3089  156  1061.762   25.935  1108.763   160.012\n",
       "47745 3089  157  1804.041  175.430  1892.821   361.565\n",
       "47746 3090  139  1491.499  172.528  1555.734       NaN\n",
       "\n",
       "[47747 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def setup_ground_truth(ground_truth_filename):\n",
    "    df = pd.read_csv(\n",
    "        ground_truth_filename,\n",
    "        header=None,\n",
    "        names=['Id', 'Frame', 'Head Valid', 'Body Valid',\n",
    "               'Head Left', 'Head Top', 'Head Right', 'Head Bottom',\n",
    "               'Body Left', 'Body Top', 'Body Right', 'Body Bottom'],\n",
    "    )\n",
    "    df['Index'] = df.index\n",
    "    df.set_index(['Index', 'Frame', 'Id'], inplace=True)\n",
    "    \n",
    "    # Selecting only the Full Body\n",
    "    df.drop(columns=[\n",
    "        'Head Valid',\n",
    "        'Head Left',\n",
    "        'Head Top',\n",
    "        'Head Right',\n",
    "        'Head Bottom'\n",
    "    ], inplace=True)\n",
    "    df.rename(columns={\n",
    "        'Body Valid': 'Valid',\n",
    "        'Body Left': 'Left',\n",
    "        'Body Top': 'Top',\n",
    "        'Body Right': 'Right',\n",
    "        'Body Bottom': 'Bottom'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Drop all non-valid rows\n",
    "    df = df[df['Valid'] == 1]\n",
    "    df.drop(columns=['Valid'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_gt = setup_ground_truth('TownCentre-groundtruth.top')\n",
    "df_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-serve",
   "metadata": {},
   "source": [
    "# API to process video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "twenty-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "def setup_input(input_filename):\n",
    "    vcap = cv2.VideoCapture(input_filename)\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "    n_frames = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(\"Frame width:\", frame_width)\n",
    "    print(\"Frame width:\", frame_height)\n",
    "    print(\"Video fps:\", fps)\n",
    "\n",
    "    return vcap, frame_width, frame_height, fps, n_frames\n",
    "\n",
    "def setup_output(output_filename, frame_width, frame_height, fps):\n",
    "    apiPreference = cv2.CAP_FFMPEG\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    vout = cv2.VideoWriter(\n",
    "        filename=output_filename,\n",
    "        apiPreference=apiPreference,\n",
    "        fourcc=fourcc,\n",
    "        fps=fps,\n",
    "        frameSize=(frame_width, frame_height),\n",
    "        params=[]\n",
    "    )\n",
    "    return vout\n",
    "\n",
    "\n",
    "# Process the video\n",
    "def process(vcap, vout, df_gt, detector, frames_to_process=100):\n",
    "    # Start app\n",
    "    window_name = \"People Detecting\"\n",
    "    cv2.startWindowThread()\n",
    "    cv2.namedWindow(window_name)\n",
    "\n",
    "    # Loop each frame\n",
    "    frame_count = 0\n",
    "    processed_frames = np.zeros(frames_to_process, dtype=object)\n",
    "\n",
    "    green = (0, 255, 0)\n",
    "    red = (255, 0 ,0)\n",
    "\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    fps_timer = [0, cv2.getTickCount()]\n",
    "    while vcap.isOpened():\n",
    "        # Read a frame\n",
    "        ret, frame = vcap.read()\n",
    "        if not ret or frame_count == frames_to_process:\n",
    "            break\n",
    "\n",
    "        # Apply the body classifier\n",
    "        bodies = detector.process(frame)\n",
    "\n",
    "        # Extract bounding boxes for any bodies identified\n",
    "        for (x, y, w, h) in bodies:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Compute and put FPS on frame\n",
    "        fps = cv2.getTickFrequency() / (fps_timer[1] - fps_timer[0]);\n",
    "        fps_timer[0] = fps_timer[1]\n",
    "        fps_timer[1] = cv2.getTickCount()\n",
    "        cv2.putText(frame,\n",
    "            text=f\"FPS: {int(fps)}\",\n",
    "            org=(frame_width -60, frame_height -5),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.3,\n",
    "            color=green,\n",
    "            thickness=1\n",
    "        );\n",
    "\n",
    "        # Save frame\n",
    "        processed_frames[frame_count] = frame\n",
    "        frame_count += 1\n",
    "\n",
    "        # Show in app\n",
    "        cv2.imshow(window_name, frame)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "    overall_elapsed_time = end - start\n",
    "    elapsed_time_per_frame = overall_elapsed_time / frame_count\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"{frame_count} frames processed in {overall_elapsed_time} seconds.\")\n",
    "    print(f\"({elapsed_time_per_frame}) seconds per frame.\")\n",
    "    print(f\"({1/elapsed_time_per_frame}) frames per second.\")\n",
    "\n",
    "    # Write processed frames to file\n",
    "    for frame in processed_frames:\n",
    "        vout.write(frame)\n",
    "\n",
    "#     print(f\"Output saved to \\\"{output_filename}\\\".\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-flashing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame: 0\n",
      "ground-truth ids: 14\n",
      "Computing the score for id_out: 0\n",
      "Computing the score for id_out: 1\n",
      "Computing the score for id_out: 2\n",
      "Computing the score for id_out: 3\n",
      "Computing the score for id_out: 4\n",
      "Computing the score for id_out: 5\n",
      "Computing the score for id_out: 6\n",
      "Computing the score for id_out: 7\n",
      "Computing the score for id_out: 8\n",
      "Computing the score for id_out: 9\n",
      "Computing the score for id_out: 10\n",
      "Computing the score for id_out: 11\n",
      "Computing the score for id_out: 12\n",
      "Computing the score for id_out: 13\n",
      "frame: 1\n",
      "ground-truth ids: 14\n",
      "Computing the score for id_out: 0\n",
      "Computing the score for id_out: 1\n",
      "Computing the score for id_out: 2\n",
      "Computing the score for id_out: 3\n",
      "Computing the score for id_out: 4\n",
      "Computing the score for id_out: 5\n",
      "Computing the score for id_out: 6\n",
      "Computing the score for id_out: 7\n",
      "Computing the score for id_out: 8\n",
      "Computing the score for id_out: 9\n",
      "Computing the score for id_out: 10\n",
      "Computing the score for id_out: 11\n",
      "Computing the score for id_out: 12\n",
      "Computing the score for id_out: 13\n"
     ]
    }
   ],
   "source": [
    "def iou(bb1, bb2):\n",
    "    \"\"\" \n",
    "    bb1: ground-truth dataframe for each video frame\n",
    "    bb2: dataframe with a single element\n",
    "    \"\"\"\n",
    "    # ref: http://jsfiddle.net/Lqh3mjr5/\n",
    "    \n",
    "    # Intersection Bouding Box\n",
    "    bbi = pd.DataFrame()\n",
    "    bbi['Left'] = np.maximum(bb1['Left'], bb2['Left'].values)\n",
    "    bbi['Right'] = np.minimum(bb1['Right'], bb2['Right'].values)\n",
    "    bbi['Bottom'] = np.minimum(bb1['Bottom'], bb2['Bottom'].values)\n",
    "    bbi['Top'] = np.maximum(bb1['Top'], bb2['Top'].values)\n",
    "    \n",
    "    # Intersection Bouding Box Area\n",
    "    bbi['x'] = bbi['Right'] - bbi['Left']\n",
    "    bbi['y'] = bbi['Bottom'] - bbi['Top']\n",
    "    bbi['x'].clip(lower=0, inplace=True)\n",
    "    bbi['y'].clip(lower=0, inplace=True)\n",
    "    bbi['Area'] = bbi['x'] * bbi['y']\n",
    "    \n",
    "    # Bouding Box 1 Area\n",
    "    bb1['x'] = bb1['Right'] - bb1['Left']\n",
    "    bb1['y'] = bb1['Bottom'] - bb1['Top']\n",
    "    bb1['Area'] = bb1['x'] * bb1['y']\n",
    "    \n",
    "    # Bouding Box 2 Area\n",
    "    bb2['x'] = bb2['Right'] - bb2['Left']\n",
    "    bb2['y'] = bb2['Bottom'] - bb2['Top']\n",
    "    bb2['Area'] = bb2['x'] * bb2['y']\n",
    "    \n",
    "    # Compute intersection area over union area\n",
    "    iou = pd.DataFrame()\n",
    "    iou['Score'] = bbi['Area'] / (bb1['Area'] + bb2['Area'] - bbi['Area'])\n",
    "    \n",
    "    iou.rename(columns={'Id': 'Id gt'}, inplace=True)\n",
    "    iou['Left'] = bb2['Left']\n",
    "    iou['Right'] = bb2['Right']\n",
    "    iou['Bottom'] = bb2['Bottom']\n",
    "    iou['Top'] = bb2['Top']\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "df_gt = setup_ground_truth('TownCentre-groundtruth.top')\n",
    "df_out = df_gt.copy()\n",
    "\n",
    "frames = [0, 1]\n",
    "id_gt = slice(None)\n",
    "i = pd.DataFrame()\n",
    "for frame in frames:\n",
    "    print('frame:', frame)\n",
    "    print('ground-truth ids:', df_gt.loc[(slice(None), frame, id_gt), :].index.size)\n",
    "    \n",
    "    id_iou_size = df_out.loc[(slice(None), frame, slice(None)), :].index.size\n",
    "    for id_out in range(id_iou_size):\n",
    "        print('Computing the score for id_out:', id_out)\n",
    "        \n",
    "        a = df_gt.loc[(slice(None), frame, id_gt), :].copy()\n",
    "        b = df_out.loc[(slice(None), frame, id_out), :].copy()\n",
    "        \n",
    "        _iou = iou(a, b)\n",
    "        if _iou.notnull().values.any():\n",
    "            _iou['Id Out'] = id_out\n",
    "            i = i.append(_iou.dropna())\n",
    "            \n",
    "    # TODO: para cada Id Out, escolher o Id com maior Score\n",
    "    \n",
    "    \n",
    "# print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-raising",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Detecting People w/ Haar Cascade Classifier\n",
    "\n",
    "The idea here is to use a Haar Cascade Classifier algorithm by loading an XML file with the pre-trained parameters for full-body detection.\n",
    "\n",
    "To understand what is Haar Cascade Classifier and how it works, there is a good basic overview from [OpenCV-Python Tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_objdetect/py_face_detection/py_face_detection.html). A more in-depth looking can be read from [Wikipedia's article Viola-Jones object detection framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework).\n",
    "\n",
    "The approach here is very simple:\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the current frame's image.\n",
    "- Run a full-body pre-trained Haar Cascade Classifier on that image, that should return a list of detected full bodies in a form of a rectangle.\n",
    "- Draw the rectangles on the image.\n",
    "- Display the image.\n",
    "- Save the processed frames into the output video.\n",
    "\n",
    "There are several similar tutorials with almost identical example codes. I particularly followed the tutorial [Computer Vision — Detecting objects using Haar Cascade Classifier, from Towards Data Science](https://towardsdatascience.com/computer-vision-detecting-objects-using-haar-cascade-classifier-4585472829a9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "demographic-spank",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame width: 640\n",
      "Frame width: 360\n",
      "Video fps: 25.0\n",
      "Done!\n",
      "100 frames processed in 2.9269297122955322 seconds.\n",
      "(0.029269297122955323) seconds per frame.\n",
      "(34.16549416267738) frames per second.\n"
     ]
    }
   ],
   "source": [
    "class haar_detector():\n",
    "    def __init__(self):\n",
    "        self.detector = cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + 'haarcascade_fullbody.xml'\n",
    "        #     cv2.data.haarcascades + 'haarcascade_upperbody.xml'\n",
    "        )\n",
    "\n",
    "    def process(self, frame):\n",
    "        bodies = self.detector.detectMultiScale(frame, 1.1, 3)\n",
    "        return bodies\n",
    "\n",
    "\n",
    "# Setup the input video\n",
    "vcap, frame_width, frame_height, fps, n_frames = setup_input('./360p_TownCentreXVID.mp4')\n",
    "\n",
    "# Setup the output video\n",
    "vout = setup_output('./output.mp4', frame_width, frame_height, fps)\n",
    "\n",
    "# Get the groud-truth data\n",
    "df_gt = setup_ground_truth('TownCentre-groundtruth.top')\n",
    "\n",
    "# Setup a detector\n",
    "detector = haar_detector()\n",
    "\n",
    "# Process the detector and save the output file\n",
    "frames_to_process = 100\n",
    "process(vcap, vout, df_gt, detector, frames_to_process)\n",
    "\n",
    "# Release\n",
    "vcap.release()\n",
    "vout.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-cable",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Detecting people with Background Subtractor\n",
    "\n",
    "The idea here is that if the camera is static, we can take advantage of that and use a background subtractor algorithm to get a mask of the moving regions, then filter it and locate the blobs. This should give us the moving objects on a static camera.\n",
    "\n",
    "The OpenCV page has a good article called [How to Use Background Subtraction Methods](https://docs.opencv.org/master/d1/dc5/tutorial_background_subtraction.html) commenting on the basics. To understand how they work and to get a performance comparison, be sure to read the article [A Comparison between Background Modelling Methods for Vehicle Segmentation in Highway Traffic Videos, by L. A. Marcomini and A. L. Cunha](https://arxiv.org/pdf/1810.02835.pdf).\n",
    "\n",
    "The approach here is also very simple:  \n",
    "- Initialize the detector as a background subtractor.\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the current frame's image.\n",
    "- Apply the detector to the image, getting a mask with the background extracted (the background is black, the non-background is white).\n",
    "- Filter with Erode, Dilate and Close to get a better separation of the detections.\n",
    "- Find each blob in the image and get its bounding box.\n",
    "- Draw the bounding box as rectangles on the image.\n",
    "- Display the image.\n",
    "- Save the processed frames into the output video.\n",
    "\n",
    "There are some tutorials around this and I particularly followed the [Object Tracking with Opencv and Python, from PySource](https://pysource.com/2021/01/28/object-tracking-with-opencv-and-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "right-elder",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame width: 640\n",
      "Frame width: 360\n",
      "Video fps: 25.0\n",
      "Done!\n",
      "100 frames processed in 0.9520258903503418 seconds.\n",
      "(0.009520258903503418) seconds per frame.\n",
      "(105.03916018838562) frames per second.\n"
     ]
    }
   ],
   "source": [
    "class background_subtraction_detector():\n",
    "    def __init__(self):\n",
    "        self.detector = cv2.createBackgroundSubtractorMOG2(\n",
    "            history=150,\n",
    "            varThreshold=50\n",
    "        )\n",
    "\n",
    "    def process(self, frame):\n",
    "        # Filter image to get people blobs\n",
    "        mask = self.detector.apply(frame)\n",
    "\n",
    "        mask[mask>125] = 255\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 7))\n",
    "        mask = cv2.erode(mask, kernel, iterations=1)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
    "        mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=5)   \n",
    "\n",
    "        # Consider each blob a person\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        bodies = []\n",
    "        for countour in contours:\n",
    "            # Calculate area and remove small elements\n",
    "            area = cv2.contourArea(countour)\n",
    "            if area > 100:\n",
    "                x, y, w, h = cv2.boundingRect(countour)\n",
    "                bodies += [(x, y, w, h)]\n",
    "\n",
    "        # Reconstruct the colors\n",
    "#         frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "        return bodies\n",
    "\n",
    "\n",
    "# Setup the input video\n",
    "vcap, frame_width, frame_height, fps, n_frames = setup_input('./360p_TownCentreXVID.mp4')\n",
    "\n",
    "# Setup the output video\n",
    "vout = setup_output('./output.mp4', frame_width, frame_height, fps)\n",
    "\n",
    "# Get the groud-truth data\n",
    "df_gt = setup_ground_truth('TownCentre-groundtruth.top')\n",
    "\n",
    "# Setup a detector\n",
    "detector = background_subtraction_detector()\n",
    "\n",
    "# Process the detector and save the output file\n",
    "frames_to_process = 100\n",
    "process(vcap, vout, df_gt, detector, frames_to_process)\n",
    "\n",
    "# Release\n",
    "vcap.release()\n",
    "vout.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-continent",
   "metadata": {},
   "source": [
    "# Detecting people using YOLO\n",
    "\n",
    "YOLO is the idea that... You Only Look Once is the idea that a CNN can process the entire image at once instead of running a detection on a sliding window through the image or a bunch of selective ROI. Instead of that, it divides the image into an SxS grid that is processed in one-pass the entire image.\n",
    "\n",
    "For each cell, a defined number of objects can be detected by using something referred to as _Dimension Clusters_, that is similar to _anchor boxes_, but has its dimensions pre-fitted to best match the best [IOU](https://en.wikipedia.org/wiki/Jaccard_index) with the bounding boxes found in the training dataset. Each detected object will have relative dimensions offset from the cluster/box used for that detection and coordinate offsets relative to the corresponding cell of the grid.\n",
    "\n",
    "Each cell contains attributes regarding each bounding box's (1) position, (2) dimension, and (3) confidence based on IOU. Also, each cell contains the list of the probability of each object classes that can be identified (3 values if trained to detect person, cat, and dog).\n",
    "\n",
    "YOLO (since version 2) trains with a random multi-scaling pre-processing. Note here that because of the nature of the non-fully connected ConvNets, the weights don't need to be changed when resizing the features. This multi-scaling process not only improves its overall precision but it means it can process images with different sizes while also gives it a parameter as a trade-off between speed (FPS) and accuracy that can be adjusted in runtime, that using OpenCV it would be referred to as `size` in the `blobFromImage()`, responsible for normalizing the input image to the DNN (read more (here)[https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works]).\n",
    "\n",
    "The image is then processed in a single convolutional network and outputs a list of bounding boxes relative to the image size.\n",
    "\n",
    "As an open-source code, currently, there are several YOLO releases, some newer from different authors, so it is quite confusing to understand, but fortunately, [Towards Data Science](https://towardsdatascience.com/yolo-v4-or-yolo-v5-or-pp-yolo-dad8e40f7109) published a good overview from version 1 to 5. Here we'll be experimenting with Darknet's YOLOv3 and AlexeyAB's YOLOv4.\n",
    "\n",
    "The approach here is also very simple:  \n",
    "- Initialize the detector as a background subtractor.\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the current frame's image.\n",
    "- Apply the detector to the image, getting a mask with the background extracted (the background is black, the non-background is white).\n",
    "- Filter with Erode, Dilate and Close to get a better separation of the detections.\n",
    "- Find each blob in the image and get its bounding box.\n",
    "- Draw the bounding box as rectangles on the image.\n",
    "- Display the image.\n",
    "- Save the processed frames into the output video.\n",
    "\n",
    "Refs:\n",
    " - [YOLOv1](https://pjreddie.com/media/files/papers/yolo_1.pdf)\n",
    " - [YOLOv2/YOLO9000](https://pjreddie.com/media/files/papers/YOLO9000.pdf)\n",
    " - [YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)\n",
    " - https://learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c\n",
    " - https://pysource.com/2019/06/27/yolo-object-detection-using-opencv-with-python\n",
    " - https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works\n",
    " - https://opencv-tutorial.readthedocs.io/en/latest/yolo/yolo.html\n",
    " - https://blog.roboflow.com/yolov5-improvements-and-evaluation/\n",
    "\n",
    "Downloads:\n",
    " - [Darknet models](https://pjreddie.com/darknet/imagenet/#pretrained)\n",
    "  - [names-file](https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names)\n",
    "  - YOLOv3:\n",
    "    - [cfg-file](https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg)\n",
    "    - [weights-file](https://pjreddie.com/media/files/yolov3.weights)\n",
    "  - YOLOv3-tiny:\n",
    "    - [cfg-file](https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3-tiny.cfg)\n",
    "    - [weights-file](https://pjreddie.com/media/files/yolov3-tiny.weights)\n",
    " - https://github.com/kiyoshiiriemon/yolov4_darknet\n",
    "  - [names-file](https://raw.githubusercontent.com/AlexeyAB/darknet/darknet_yolo_v4_pre/cfg/coco.names)\n",
    "  - YOLOv4:\n",
    "    - [cfg-file](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.cfg)\n",
    "     - [weights-file](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "  - YOLOv4-tiny:\n",
    "    - [cfg-file](https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny.cfg)\n",
    "    - [weights-file](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "practical-reconstruction",
   "metadata": {
    "code_folding": [
     63,
     111
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame width: 640\n",
      "Frame width: 360\n",
      "Video fps: 25.0\n",
      "80 classes loaded: []\n",
      "Done!\n",
      "100 frames processed in 1.8788604736328125 seconds.\n",
      "(0.018788604736328124) seconds per frame.\n",
      "(53.22374992893863) frames per second.\n"
     ]
    }
   ],
   "source": [
    "class yolo_detector():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Define YOLO files to load\n",
    "        # weights_file, cfg_file, names_file = \"pretrained_yolo/v3/yolov3-tiny.weights\", \"pretrained_yolo/v3/yolov3-tiny.cfg\", \"pretrained_yolo/v3/coco.names\"\n",
    "        weights_file, cfg_file, names_file = \"pretrained_yolo/v4/yolov4-tiny.weights\", \"pretrained_yolo/v4/yolov4-tiny.cfg\", \"pretrained_yolo/v4/coco.names\"\n",
    "\n",
    "        # YOLO and DNN Configs\n",
    "        # ref: https://docs.opencv.org/4.5.1/d6/d0f/group__dnn.html\n",
    "        self.mean = (0, 0, 0)  # YOLO doesn't use subtraction.\n",
    "        self.scale_factor = 1 / (255)  # the colorspace is normalized to match values from 0 to 1, so for 8 bits depth it should be 1/255.\n",
    "        self.blob_size = tuple([128*2]*2)  # this will impacat on precision over FPS. Smaller means faster. 320x320 is a common value.\n",
    "        self.confidence_threshold = 0.3  # 0 means no threshold. 0.5 is a common value.\n",
    "        self.supression_threshold = 0.4  # 1 means no supression. 0.4 is a common value.\n",
    "\n",
    "        dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_CPU, cv2.dnn.DNN_BACKEND_DEFAULT\n",
    "        # dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_CPU, cv2.dnn.DNN_BACKEND_OPENCV\n",
    "        # dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_OPENCL, cv2.dnn.DNN_BACKEND_DEFAULT\n",
    "        # dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_OPENCL, cv2.dnn.DNN_BACKEND_OPENCV\n",
    "        # dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_CUDA, cv2.dnn.DNN_BACKEND_CUDA\n",
    "        # dnn_target, dnn_backend = cv2.dnn.DNN_TARGET_VULKAN, cv2.dnn.DNN_BACKEND_VKCOM\n",
    "\n",
    "        # Load YOLO\n",
    "        net = cv2.dnn.readNet(weights_file, cfg_file)\n",
    "        net.setPreferableBackend(dnn_backend)\n",
    "        net.setPreferableTarget(dnn_target)\n",
    "\n",
    "        classes = []\n",
    "        with open(names_file, \"r\") as f:\n",
    "            self.classes = [line.strip() for line in f.readlines()]\n",
    "            print(f\"{len(self.classes)} classes loaded:\", end=' ')\n",
    "            print(classes)\n",
    "        layer_names = net.getLayerNames()\n",
    "        # output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "        self.output_layers = net.getUnconnectedOutLayersNames()\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "        \n",
    "        self.classes = classes\n",
    "        self.net = net\n",
    "\n",
    "    def process(self, frame):\n",
    "        # Detecting objects\n",
    "        blob = cv2.dnn.blobFromImage(frame,\n",
    "            scalefactor=self.scale_factor,\n",
    "            size=self.blob_size,\n",
    "            mean=self.mean,\n",
    "            swapRB=True,\n",
    "            crop=False,\n",
    "    #         ddepth=cv2.CV_32F\n",
    "        )\n",
    "        self.net.setInput(blob)\n",
    "        output_blobs = self.net.forward(self.output_layers)\n",
    "\n",
    "        # Extract bounding boxes for any object detected\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for output_blob in output_blobs:\n",
    "            for detection in output_blob:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > self.confidence_threshold:\n",
    "                    center_x, center_y, w, h = detection[:4] * np.array(\n",
    "                        [frame_width, frame_height, frame_width, frame_height])\n",
    "                    x = center_x - (w / 2)\n",
    "                    y = center_y - (h / 2)\n",
    "                    boxes.append([int(x), int(y), int(w), int(h)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Remove coincidental detections\n",
    "        indices = cv2.dnn.NMSBoxes(\n",
    "            bboxes=boxes, \n",
    "            scores=confidences, \n",
    "            score_threshold=self.confidence_threshold, \n",
    "            nms_threshold=self.supression_threshold\n",
    "        )\n",
    "        \n",
    "        bodies = []\n",
    "        # Showing informations on the screen\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indices:\n",
    "                bodies += [boxes[i]]\n",
    "#                 x, y, w, h = boxes[i]\n",
    "#                 label = classes[class_ids[i]]\n",
    "#                 confidence = round(confidences[i]*100)\n",
    "#                 color = colors[class_ids[i]]\n",
    "#                 cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "#                 cv2.putText(\n",
    "#                     frame,\n",
    "#                     text=f\"{label} : {confidence}%\",\n",
    "#                     org=(x, y-5),\n",
    "#                     fontFace=font,\n",
    "#                     fontScale=0.3,\n",
    "#                     color=color,\n",
    "#                     thickness=1\n",
    "#                 )\n",
    "\n",
    "        return bodies\n",
    "\n",
    "\n",
    "# Setup the input video\n",
    "vcap, frame_width, frame_height, fps, n_frames = setup_input('./360p_TownCentreXVID.mp4')\n",
    "\n",
    "# Setup the output video\n",
    "vout = setup_output('./output.mp4', frame_width, frame_height, fps)\n",
    "\n",
    "# Get the groud-truth data\n",
    "df_gt = setup_ground_truth('TownCentre-groundtruth.top')\n",
    "\n",
    "# Setup a detector\n",
    "detector = yolo_detector()\n",
    "\n",
    "# Process the detector and save the output file\n",
    "frames_to_process = 100\n",
    "df_out = process(vcap, vout, detector, frames_to_process)\n",
    "\n",
    "# Release\n",
    "vcap.release()\n",
    "vout.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-combination",
   "metadata": {},
   "source": [
    "next: \n",
    "- Scaled-YOLOv4?\n",
    "- Training with custom dataset?\n",
    "- YOLOv5 with PyTorch?\n",
    "- Semantic Segmentation?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
