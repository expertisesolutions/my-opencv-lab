{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infrared-raising",
   "metadata": {},
   "source": [
    "## Tracking People\n",
    "\n",
    "This is an initial experiment for learning purpose on using OpenCV for people tracking on videos.\n",
    "\n",
    "The OpenCV API give us several ways to detect objects in images, but what if we want to count how many people are walking by a road monitored with a security camera?\n",
    "\n",
    "Since we probably don't want to identify who is each person because of privacy regulations (and also because of that would be waste of resources), we need a way to get track of each person as an unkown, or a simple blob.\n",
    "\n",
    "To achieve that, several tracking algorithms can be explored. The main components of the tracking algorithms are a model of its features, a model of its movement, and a search methodology. \n",
    "\n",
    "To give an idea, some naive implementation could purely run a search on the entire image for the near correlation peak of the extracted features, while some other can model the object's movement, like computing its speed and generating a probability map as a prediction for the next position in which it will search for the best correlation, also, a third one could update the extracted features at each iteration, or a fourth could use a deep-learning approach, that is each day reaching the feasibility for more common hardwares.\n",
    "\n",
    "To be robust, the algorithms has to perform well under several conditions like object oclusion, image blur, changes in luminosity and noise.\n",
    "\n",
    "To learn about the tracking algorithms my suggestion is to read [this](https://faculty.ucmerced.edu/mhyang/papers/spie11a.pdf) and [this](https://arxiv.org/pdf/1812.07368.pdf) articles.\n",
    "\n",
    "Also a comparison between the tracking algorithms available in OpenCV is shown in [this](https://www.researchgate.net/publication/344247798_Single_Object_Trackers_in_OpenCV_A_Benchmark) article.\n",
    "\n",
    "\n",
    "### Automating the Detection\n",
    "\n",
    "With OpenCV it is fairly simple to load and use one of the several tracking algorithms available on the `opencv-contrib-python 3.4` package, like the very fast `Mosse`, but in the first place how to identify the person to be tracked?\n",
    "\n",
    "Several tutorials about tracking focus only on tracking, leaving it to the user to use an embedded tool to draw a rectangle representing the Region Of Interest (ROI), used as input to the tracker algorithm.\n",
    "\n",
    "But what if we want it to be automated? Well, I think that is where we get some fun here.\n",
    "\n",
    "An easy strategy could be to narrow the detection by cropping the image to a small zone, like a strip of a crosswalk, where the flow will be passing through, and only there, to do the detection that will be the targets of the people tracker. Unlikely cars, the flow of the people can be very caotic compared to a car lane, where I think this strategy will be very succesefull.\n",
    "\n",
    "That being said, if we will use the entire image for detection of people, we need to write an integration between the person that was already detected and is being tracked, with the person that is not being tracked and is now detected.\n",
    "\n",
    "The most simplistic way I can think is to check if the centroid of the newly detected is outside the rectangle of any tracked person in that moment. \n",
    "\n",
    "### The first experiment\n",
    "\n",
    "The approach here is very simple:\n",
    "\n",
    "We are trying to combine the output from an object detection algorithm (that is expensive to run), with an object tracking algorithm (that is cheaper to run) with the objective to keep track of each person from the first to the last frame that person keeps in. A description of the step-by-step is shown below:\n",
    "\n",
    "- Open the input video, get its shape and FPS.\n",
    "- Setup the output video with the same shape and FPS from the input video.\n",
    "- From the input video, get the image of the frame.\n",
    "- Run a full or upper body pre-trained Haar Cascade Classifier on that image, that should return a list of detected full body in a form of a rectangle.\n",
    "- From that list, try to identify which are new and which was already being tracked.\n",
    "- Each person has an _ID_, a _failure counter_ and its own _tracker algorithm object_ that will be used in the next frames to maintain the right track.\n",
    "- We set a threshold level for the person's failure counter so we can stop tracking that person.\n",
    "- For each tracked person, draws its rectangle with a text containing its ID/failure counter on top of it, also draws its centroid.\n",
    "Display the image.\n",
    "Save the processed frames into the output video.\n",
    "\n",
    "\n",
    "## References:  \n",
    "- https://faculty.ucmerced.edu/mhyang/papers/spie11a.pdf\n",
    "- https://arxiv.org/pdf/1812.07368.pdf\n",
    "- https://learnopencv.com/object-tracking-using-opencv-cpp-python/  \n",
    "- https://github.com/adipandas/multi-object-tracker  \n",
    "- https://github.com/stephanj/basketballVideoAnalysis  \n",
    "- https://gist.github.com/harshilpatel312/ff08b49fd71a3eeaeb209c91de3dfde1  \n",
    "- https://www.robots.ox.ac.uk/~joao/circulant/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "iraqi-addiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Trackers:\n",
      "\t - Tracker\n",
      "\t - TrackerCSRT\n",
      "\t - TrackerCSRT_Params\n",
      "\t - TrackerCSRT_create\n",
      "\t - TrackerGOTURN\n",
      "\t - TrackerGOTURN_Params\n",
      "\t - TrackerGOTURN_create\n",
      "\t - TrackerKCF\n",
      "\t - TrackerKCF_CN\n",
      "\t - TrackerKCF_CUSTOM\n",
      "\t - TrackerKCF_GRAY\n",
      "\t - TrackerKCF_Params\n",
      "\t - TrackerKCF_create\n",
      "\t - TrackerMIL\n",
      "\t - TrackerMIL_Params\n",
      "\t - TrackerMIL_create\n",
      "\t - legacy_MultiTracker\n",
      "\t - legacy_Tracker\n",
      "\t - legacy_TrackerBoosting\n",
      "\t - legacy_TrackerCSRT\n",
      "\t - legacy_TrackerKCF\n",
      "\t - legacy_TrackerMIL\n",
      "\t - legacy_TrackerMOSSE\n",
      "\t - legacy_TrackerMedianFlow\n",
      "\t - legacy_TrackerTLD\n",
      "\t - rapid_OLSTracker\n",
      "\t - rapid_Tracker\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "print('Available Trackers:')\n",
    "for d in dir(cv2):\n",
    "    if 'Tracker' in d:\n",
    "        print('\\t -',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "painted-rouge",
   "metadata": {
    "code_folding": [
     13,
     37,
     61,
     65,
     86,
     96,
     97,
     101,
     113,
     122,
     150
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2  # or opencv-python\n",
    "import time\n",
    "\n",
    "\n",
    "class PersonTracker:\n",
    "    def __init__(self, id, frame, bbox, tracking_algorithm, fails_limit, color=(0,255,0), debug=False):\n",
    "        self.debug = debug\n",
    "        self.fails_limit = fails_limit\n",
    "        \n",
    "        bbox = tuple(bbox.astype(int))\n",
    "\n",
    "        # Select our tracking algorithm and create our multi tracker\n",
    "        OPENCV_OBJECT_TRACKERS = {\n",
    "#             \"boosting\": cv2.TrackerBoosting_create,  # opencv 3.4\n",
    "            \"mil\": cv2.TrackerMIL_create,\n",
    "            \"kcf\": cv2.TrackerKCF_create,\n",
    "#             \"tld\": cv2.TrackerTLD_create,  # opencv 3.4\n",
    "#             \"medianflow\": cv2.TrackerMedianFlow_create,  # opencv 3.4\n",
    "            \"goturn\": cv2.TrackerGOTURN_create,\n",
    "#             \"mosse\": cv2.TrackerMOSSE_create,  # opencv 3.4\n",
    "            \"csrt\": cv2.TrackerCSRT_create,\n",
    "        }\n",
    "        self.tracker = OPENCV_OBJECT_TRACKERS[tracking_algorithm]()\n",
    "        self.tracker.init(frame, bbox)\n",
    "        self.active, bbox = self.tracker.update(frame)\n",
    "        bbox = tuple(np.array(bbox, dtype=int))\n",
    "               \n",
    "        if self.active == True:\n",
    "            print(f\"New person tracker added with id {id}.\")\n",
    "            self.tracking_algorithm = tracking_algorithm\n",
    "            self.id = int(id)\n",
    "            self.centroid = self.get_centroid(bbox)\n",
    "            self.fails = int(0)\n",
    "            self.color = color\n",
    "            self.bbox = bbox\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"id: {self.id}, fails: {self.fails}, active: {self.active}, bbox: {self.bbox}\"\n",
    "\n",
    "    def update(self, frame):\n",
    "        if self.active == False:\n",
    "            return False\n",
    "\n",
    "        retval, bbox = self.tracker.update(frame)\n",
    "        bbox = tuple(np.array(bbox, dtype=int))\n",
    "        \n",
    "        stucked = (self.bbox[0] == bbox[0]) and (self.bbox[1] == bbox[1])\n",
    "        if (retval == False) or (stucked == True):\n",
    "            self.fails += int(1)\n",
    "        else:\n",
    "            self.fails = int(0)\n",
    "            self.active = True\n",
    "        self.bbox = bbox\n",
    "\n",
    "        print(f\"Updated id {self.id}, fails: {self.fails}, bbox: {self.bbox} -> {bbox}\")\n",
    "        if self.fails >= self.fails_limit:\n",
    "            self.remove()\n",
    "\n",
    "        return self.active\n",
    "\n",
    "    def remove(self):\n",
    "        print(f\"Person tracker with id {self.id} removed.\")\n",
    "        self.active = False\n",
    "\n",
    "    def get_centroid(self, bbox):\n",
    "        (x, y, w, h) = bbox\n",
    "        xc = int(x + (w * 0.5))\n",
    "        yc = int(y + (h * 0.5))\n",
    "        return xc, yc\n",
    "        \n",
    "    def draw(self, frame):\n",
    "        (x, y, w, h) = np.array(self.bbox, dtype=int)\n",
    "        cv2.rectangle(frame,\n",
    "            pt1=(x, y),\n",
    "            pt2=(x + w, y + h),\n",
    "            color=self.color,\n",
    "            thickness=1\n",
    "        )\n",
    "        centroid = self.get_centroid(self.bbox)\n",
    "        cv2.circle(frame,\n",
    "            center=centroid,\n",
    "            radius=2,\n",
    "            color=self.color,\n",
    "            thickness=1\n",
    "        )\n",
    "        cv2.putText(frame,\n",
    "            text=f\"{(int(self.id))}/{int(self.fails)}\",\n",
    "            org=(x,y),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.4,\n",
    "            color=self.color,\n",
    "            thickness=1\n",
    "        );\n",
    "\n",
    "\n",
    "class PeopleTracker:\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        self.trackers = list()\n",
    "\n",
    "    def __str__(self):\n",
    "        ret = \"\"\n",
    "        for i in range(len(self.trackers)):\n",
    "            ret += f\"{self.trackers[i]}\\n\"\n",
    "        return ret[:-1]\n",
    "\n",
    "    def update(self, frame):\n",
    "        if len(self.trackers) == 0:\n",
    "            return False\n",
    "        for i in range(len(self.trackers)):\n",
    "            retval = self.trackers[i].update(frame)\n",
    "\n",
    "    def isPointInsideRect(self, point, rect) -> bool:\n",
    "        (x, y) = point\n",
    "        (x1, y1, w, h) = rect\n",
    "        (x2, y2) = (x1 + w, y1 + h)\n",
    "        return (x1 < x < x2) and (y1 < y < y2)\n",
    "\n",
    "    def add(self, frame, bbox, tracking_algorithm=\"kcf\", fails_limit=25):\n",
    "        id = len(self.trackers) + 1\n",
    "        \n",
    "        tracker = PersonTracker(\n",
    "            id, frame, bbox, tracking_algorithm, fails_limit\n",
    "        )\n",
    "        \n",
    "        # Here is the integration between Detection and Tracking:\n",
    "        # We are only adding a new person if its centroid resides outside\n",
    "        # any other active tracked person's bbox, otherwise we use that\n",
    "        # detecion to update the already tracked person. Note that this \n",
    "        # is not the best idea to deal with oclusion.\n",
    "        isNew = True\n",
    "        if tracker.active == True:\n",
    "            for i in range(len(self.trackers)):\n",
    "                if self.isPointInsideRect(\n",
    "                    tracker.centroid, self.trackers[i].bbox\n",
    "                ):\n",
    "                    # Reset their fails\n",
    "                    self.trackers[i].fails = 0\n",
    "                    # Reactivate with this new tracker if it is inactive\n",
    "                    if self.trackers[i].active == False:\n",
    "                        tracker.id = i\n",
    "                        self.trackers[i].tracker = tracker\n",
    "                    isNew = False\n",
    "\n",
    "        if isNew == True:\n",
    "            self.trackers.append(tracker)\n",
    "\n",
    "        return retval, id\n",
    "\n",
    "    def remove(self, id):\n",
    "        self.trackers[id].remove()\n",
    "\n",
    "    def draw(self, frame):\n",
    "        if len(self.trackers) == 0:\n",
    "            return False\n",
    "        for i in range(len(self.trackers)):\n",
    "            if self.trackers[i].active == True:\n",
    "                self.trackers[i].draw(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "demographic-spank",
   "metadata": {
    "code_folding": [
     21,
     86
    ]
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-43-9cb0c96936b1>, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-9cb0c96936b1>\"\u001b[0;36m, line \u001b[0;32m78\u001b[0m\n\u001b[0;31m    [None]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Open the input video capture\n",
    "# minSize, maxSize, input_filename = ((100,100), (120,120), './1080p_TownCentreXVID.mp4')\n",
    "# minSize, maxSize, input_filename = (None, (120,120), './720p_TownCentreXVID.mp4')\n",
    "# minSize, maxSize, input_filename = ((20,20), (80,80), './480p_TownCentreXVID.mp4')\n",
    "minSize, maxSize, input_filename = ((5,10), (60,40), './360p_TownCentreXVID.mp4')\n",
    "vcap = cv2.VideoCapture(input_filename)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(vcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(vcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = vcap.get(cv2.CAP_PROP_FPS)\n",
    "n_frames = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(\"Frame width:\", frame_width)\n",
    "print(\"Frame width:\", frame_height)\n",
    "print(\"Video fps:\", fps)\n",
    "\n",
    "# Setup the output video file\n",
    "output_filename = './output.mp4'\n",
    "apiPreference = cv2.CAP_FFMPEG\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "vout = cv2.VideoWriter(\n",
    "    filename=output_filename,\n",
    "    apiPreference=apiPreference,\n",
    "    fourcc=fourcc,\n",
    "    fps=fps,\n",
    "    frameSize=(frame_width, frame_height),\n",
    ")\n",
    "\n",
    "print(f\"Processing \\\"{input_filename}\\\" ({int(n_frames)} frames)...\")\n",
    "\n",
    "# Create our body classifier\n",
    "detector = cv2.CascadeClassifier(\n",
    "#     cv2.data.haarcascades + 'haarcascade_fullbody.xml'\n",
    "    cv2.data.haarcascades + 'haarcascade_upperbody.xml'\n",
    ")\n",
    "# Create our People Tracker\n",
    "people = PeopleTracker()\n",
    "\n",
    "# # Start app\n",
    "window_name = \"People Tracking\"\n",
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(window_name)\n",
    "\n",
    "# Loop each frame\n",
    "frame_count = 0\n",
    "frames_to_process = 1000\n",
    "processed_frames = np.zeros(frames_to_process, dtype=object)\n",
    "\n",
    "green = (0, 255, 0)\n",
    "red = (255, 0 ,0)\n",
    "\n",
    "# start timer\n",
    "start = time.time()\n",
    "keyframe_interval = 10\n",
    "fps_timer = [0, cv2.getTickCount()]\n",
    "while vcap.isOpened():\n",
    "    # Read a frame\n",
    "    retval, frame = vcap.read()\n",
    "    if not retval or frame_count == frames_to_process:\n",
    "        break\n",
    "    \n",
    "    # Use the classifier to detect new people\n",
    "    if frame_count % keyframe_interval == 0:\n",
    "        bboxes = detector.detectMultiScale(frame,\n",
    "            scaleFactor=1.05,\n",
    "            minNeighbors=2,\n",
    "            minSize=minSize,\n",
    "            maxSize=maxSize\n",
    "        )\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            people.add(frame, bbox, fails_limit=50,\n",
    "                tracking_algorithm=\"kcf\",\n",
    "#                 tracking_algorithm=\"csrt\",\n",
    "#                 tracking_algorithm=\"mil\",\n",
    "#                 tracking_algorithm=\"goturn\",\n",
    "                [None]\n",
    "            )\n",
    "            \n",
    "    people.update(frame)\n",
    "    \n",
    "    people.draw(frame)\n",
    "    \n",
    "    # Compute and put FPS on frame\n",
    "    fps = cv2.getTickFrequency() / (fps_timer[1] - fps_timer[0]);\n",
    "    fps_timer[0] = fps_timer[1]\n",
    "    fps_timer[1] = cv2.getTickCount()\n",
    "    cv2.putText(frame,\n",
    "        text=f\"FPS: {int(fps)}\",\n",
    "        org=(frame_width -60, frame_height -5),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.3,\n",
    "        color=green,\n",
    "        thickness=1\n",
    "    );\n",
    "\n",
    "    # Save frame\n",
    "    processed_frames[frame_count] = frame\n",
    "    frame_count += 1\n",
    "\n",
    "    # Show in app\n",
    "    cv2.imshow(window_name, frame)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# end timer\n",
    "end = time.time()\n",
    "overall_elapsed_time = end - start\n",
    "elapsed_time_per_frame = overall_elapsed_time / frame_count\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"{frame_count} frames processed in {overall_elapsed_time} seconds.\")\n",
    "print(f\"({elapsed_time_per_frame}) seconds per frame.\")\n",
    "print(f\"({1/elapsed_time_per_frame}) frames per second.\")\n",
    "\n",
    "# Write processed frames to file\n",
    "for frame in processed_frames:\n",
    "    vout.write(frame)\n",
    "\n",
    "print(f\"Output saved to \\\"{output_filename}\\\".\")\n",
    "\n",
    "vcap.release()\n",
    "vout.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
